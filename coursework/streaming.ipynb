{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 23:29:23.886990: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-25 23:29:23.897966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735162163.911083   33088 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735162163.914454   33088 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-25 23:29:23.927606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType, StringType, StructType, StructField\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, concat, udf, col, array\n",
    "from tensorflow.keras.models import load_model\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/25 23:29:24 WARN Utils: Your hostname, DESKTOP-SMHNFU4 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/25 23:29:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/25 23:29:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = SparkSession.builder.appName(\"Traffic Signs Classification Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.pyspark.memory\", \"12g\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"128\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"./tmp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dataset_path = \"./dataset\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Width\", IntegerType(), True),\n",
    "    StructField(\"Height\", IntegerType(), True),\n",
    "    StructField(\"Roi.X1\", IntegerType(), True),\n",
    "    StructField(\"Roi.Y1\", IntegerType(), True),\n",
    "    StructField(\"Roi.X2\", IntegerType(), True),\n",
    "    StructField(\"Roi.Y2\", IntegerType(), True),\n",
    "    StructField(\"ClassId\", IntegerType(), True),\n",
    "    StructField(\"Path\", StringType(), True)\n",
    "])\n",
    "\n",
    "input_stream = spark.readStream.option(\"header\", \"true\").schema(schema).csv(os.path.join(dataset_path, \"streaming\"))\n",
    "input_stream = input_stream.withColumn(\"Path\", concat(lit(dataset_path + \"/\"), input_stream[\"Path\"])).dropna(subset=[\"ClassId\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.image_to_vector(img_features)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_image(img_path, roi_x1, roi_y1, roi_x2, roi_y2):\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        cropped_img = img.crop((roi_x1, roi_y1, roi_x2, roi_y2))\n",
    "        resized_img = np.array(cropped_img.resize((32, 32), resample=Image.Resampling.LANCZOS))\n",
    "        return (resized_img.flatten() / 255.0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {img_path}: {e}\")\n",
    "        return [0.0] * (32 * 32 * 3)\n",
    "\n",
    "def image_to_vector(img_features):\n",
    "    return Vectors.dense(img_features)\n",
    "\n",
    "spark.udf.register(\"process_image\", process_image, ArrayType(FloatType()))\n",
    "spark.udf.register(\"image_to_vector\", image_to_vector, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lrModel = PipelineModel.load(\"best-model-lr\")\n",
    "rfModel = PipelineModel.load(\"best-model-rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 23:29:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/12/25 23:29:40 WARN DAGScheduler: Broadcasting large task binary with size 1289.0 KiB\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 444 Î¼s, total: 15.1 ms\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "lrPredictionsStream = lrModel.transform(input_stream)\n",
    "\n",
    "outputStream = lrPredictionsStream.select(\"Path\", \"ClassId\", \"prediction\").writeStream.option(\"path\", \"output/lr\").outputMode(\"append\").trigger(once=True).option(\"header\", \"true\").format(\"csv\").start()\n",
    "%time outputStream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 23:30:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/12/25 23:30:37 WARN DAGScheduler: Broadcasting large task binary with size 318.3 MiB\n",
      "Current mem limits: 12884901888 of max 12884901888                  (0 + 5) / 5]\n",
      "\n",
      "Current mem limits: 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: 12884901888 of max 12884901888\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.67 ms, sys: 9.77 ms, total: 14.4 ms\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "rfPredictionsStream = rfModel.transform(input_stream)\n",
    "\n",
    "outputStream = rfPredictionsStream.select(\"Path\", \"ClassId\", \"prediction\").writeStream.option(\"path\", \"output/rf\").outputMode(\"append\").trigger(once=True).option(\"header\", \"true\").format(\"csv\").start()\n",
    "%time outputStream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too slow\n",
    "\n",
    "# tflite_model_path = \"best_model.tflite\"\n",
    "# broadcast_model_path = spark.sparkContext.broadcast(tflite_model_path)\n",
    "\n",
    "# def predict_image_class(img_path, roi_x1, roi_y1, roi_x2, roi_y2):\n",
    "#   try:\n",
    "#     img = Image.open(img_path)\n",
    "#     cropped_img = img.crop((roi_x1, roi_y1, roi_x2, roi_y2))\n",
    "#     resized_img = np.array(cropped_img.resize((32, 32), resample=Image.Resampling.LANCZOS)) / 255.0\n",
    "#     resized_img = np.expand_dims(resized_img, axis=0).astype(np.float32)\n",
    "    \n",
    "#     model_path = broadcast_model_path.value\n",
    "#     interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "#     interpreter.allocate_tensors()\n",
    "#     input_details = interpreter.get_input_details()\n",
    "#     output_details = interpreter.get_output_details()\n",
    "    \n",
    "    \n",
    "#     interpreter.set_tensor(input_details[0]['index'], resized_img)\n",
    "#     interpreter.invoke()\n",
    "#     output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "#     predicted_class = np.argmax(output_data)\n",
    "#     return int(predicted_class)\n",
    "\n",
    "    \n",
    "#   except Exception as e:\n",
    "#     print(f\"Error processing image {img_path}: {e}\")\n",
    "#     raise e\n",
    "  \n",
    "  \n",
    "# predict_image_class_udf = udf(predict_image_class, IntegerType())\n",
    "\n",
    "# kerasPredictionsStream = input_stream.withColumn(\"prediction\", predict_image_class_udf(\"Path\", \"`Roi.X1`\", \"`Roi.Y1`\", \"`Roi.X2`\", \"`Roi.Y2`\"))\n",
    "\n",
    "# outputStream = kerasPredictionsStream.select(\"Path\", \"ClassId\", \"prediction\").writeStream.option(\"path\", \"output/keras\").outputMode(\"append\").trigger(once=True).format(\"csv\").start()\n",
    "# outputStream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 23:31:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n",
      "Current mem limits: -1 of max -1\n",
      "\n",
      "Setting mem limits to 12884901888 of max 12884901888\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.9 s, sys: 7.6 s, total: 59.5 s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    model_path = \"best_model.tflite\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    batch_pd: DataFrame = batch_df.toPandas()\n",
    "\n",
    "    def predict_image_class(row):\n",
    "        try:\n",
    "            img_path = row[\"Path\"]\n",
    "            roi_x1, roi_y1, roi_x2, roi_y2 = row[\"Roi.X1\"], row[\"Roi.Y1\"], row[\"Roi.X2\"], row[\"Roi.Y2\"]\n",
    "            \n",
    "            img = Image.open(img_path)\n",
    "            cropped_img = img.crop((roi_x1, roi_y1, roi_x2, roi_y2))\n",
    "            resized_img = np.array(cropped_img.resize((32, 32), resample=Image.Resampling.LANCZOS)) / 255.0\n",
    "            resized_img = np.expand_dims(resized_img, axis=0).astype(np.float32)\n",
    "\n",
    "            interpreter.set_tensor(input_details[0]['index'], resized_img)\n",
    "            interpreter.invoke()\n",
    "            \n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            predicted_class = int(np.argmax(output_data))\n",
    "\n",
    "            return predicted_class\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    batch_pd[\"prediction\"] = batch_pd.apply(predict_image_class, axis=1)\n",
    "    batch_pd = batch_pd.filter(items=[\"Path\", \"ClassId\", \"prediction\"])\n",
    "    result_df = spark.createDataFrame(batch_pd)\n",
    "    result_df.write.format(\"csv\").option(\"header\", \"true\").option(\"path\", \"output/keras\").mode(\"append\").save()\n",
    "\n",
    "query = input_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "%time query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total count\n",
      "lr 63150\n",
      "rf 63150\n",
      "keras 63150\n",
      "\n",
      "\n",
      "Accuracy\n",
      "lr 0.8592240696753761\n",
      "rf 0.8188440221694379\n",
      "keras 0.97458432304038\n"
     ]
    }
   ],
   "source": [
    "lr_results = spark.read.csv(\"output/lr\", header=True)\n",
    "rf_results = spark.read.csv(\"output/rf\", header=True)\n",
    "keras_results = spark.read.csv(\"output/keras\", header=True)\n",
    "\n",
    "lr_total = lr_results.count()\n",
    "rf_total = rf_results.count()\n",
    "keras_total = keras_results.count()\n",
    "\n",
    "print(\"\\n\\nTotal count\")\n",
    "print(f\"lr {lr_total}\\nrf {rf_total}\\nkeras {keras_total}\")\n",
    "\n",
    "lr_accuracy = lr_results.filter(lr_results[\"ClassId\"] == lr_results[\"prediction\"].cast(IntegerType())).count() / lr_total\n",
    "rf_accuracy = rf_results.filter(rf_results[\"ClassId\"] == rf_results[\"prediction\"].cast(IntegerType())).count() / rf_total\n",
    "keras_accuracy = keras_results.filter(keras_results[\"ClassId\"] == keras_results[\"prediction\"]).count() / keras_total\n",
    "\n",
    "print(\"\\n\\nAccuracy\")\n",
    "print(f\"lr {lr_accuracy}\\nrf {rf_accuracy}\\nkeras {keras_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
